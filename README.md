# deep-learning-lab-1

Deep learning is an approach to machine learning characterized by deep stacks of computations. 
This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical 
patterns found in the most challenging real-world datasets

### Data Preprocessing

### Scaling and Normalization
- In `scaling` you're changing the range of your data
- In `normalization` you're changing the shape of the distribution of your data


Scaling means to transform data so that it fits within a specific scale, like 0-100, 0-1. You want to scale data when you're
using methods based on measures of how far apart data points are, like <b>support vector machines</b> 

Normalization is more radical transformation. The point of normalization is to change observations so that they can 
be described as a normal distribution. Normal distribution also known as the Gaussian distribution.

Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any
parameter tuning (it is "self tuning", in a sense). Adam is a great general-purpose optimizer.